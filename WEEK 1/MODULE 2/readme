MODULE 2 LANGSMITH, USED GROQ API AND EMBEDDINGS

LESSON 1 DATASETS - The video shows how to create and use datasets in LangSmith for offline testing. It covers importing CSVs, adding examples manually or from real traces, tagging versions, splitting datasets for specific tests, and sharing/exporting them. The goal is to build reliable “golden sets” that help track and improve app performance over time.

LESSON 2 EVALUATORS - The video explains how to use evaluators in LangSmith to measure AI performance with metrics like accuracy and hallucination. It shows how to write custom evaluators, use LLMs as judges, run automatic evaluations on datasets, and track RAG metrics such as document relevance and helpfulness. It also covers tagging inputs, running code-based evaluators, and applying them to past runs for stronger evaluation.

LESSON 3 EXPERIMENTS - The video shows how to run experiments in LangSmith by combining datasets and evaluators. It covers setting up experiments in the UI or SDK, testing full or partial datasets, tweaking models and parameters for comparison, repeating runs for consistency, managing concurrency, adding metadata for filtering, and tracking results in the experiments panel over time. Used app.py and there are a few errors and also defined the reason for error near them.

LESSON 4 ANALYSING EXPERIMENT RESULTS - The video shows how to use the LangSmith UI to compare experiments. It covers filtering by model or metadata, checking inputs and outputs, reviewing evaluator feedback, and comparing versions side by side to see trade-offs like latency vs accuracy. It highlights how regular experimentation improves AI apps over time. NO CODE GIVEN TO IMPLEMENT.

LESSON 5 PAIRWISE - The video explains pairwise evaluation, where outputs from two experiments are compared to judge which is better. This approach is helpful when direct scoring is hard but side-by-side comparison is easier.
