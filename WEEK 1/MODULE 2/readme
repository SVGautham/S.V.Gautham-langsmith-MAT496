MODULE 2 LANGSMITH, USED GROQ API AND EMBEDDINGS

LESSON 1 DATASETS - The video shows how to create and use datasets in LangSmith for offline testing. It covers importing CSVs, adding examples manually or from real traces, tagging versions, splitting datasets for specific tests, and sharing/exporting them. The goal is to build reliable “golden sets” that help track and improve app performance over time.

LESSON 2 EVALUATORS - The video explains how to use evaluators in LangSmith to measure AI performance with metrics like accuracy and hallucination. It shows how to write custom evaluators, use LLMs as judges, run automatic evaluations on datasets, and track RAG metrics such as document relevance and helpfulness. It also covers tagging inputs, running code-based evaluators, and applying them to past runs for stronger evaluation.

LESSON 3 EXPERIMENTS - The video shows how to run experiments in LangSmith by combining datasets and evaluators. It covers setting up experiments in the UI or SDK, testing full or partial datasets, tweaking models and parameters for comparison, repeating runs for consistency, managing concurrency, adding metadata for filtering, and tracking results in the experiments panel over time.
