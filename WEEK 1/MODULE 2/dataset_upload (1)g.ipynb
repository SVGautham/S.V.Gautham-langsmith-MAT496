{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM4uJnqOM_Nk"
      },
      "source": [
        "# Dataset Upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYgIYKyxM_Nm"
      },
      "source": [
        "In addition to creating and editing Datasets in the LangSmith UI, you can also create and edit datasets with the LangSmith SDK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alDJSRkEM_Nn"
      },
      "source": [
        "Let's go ahead an upload a list of examples that we have from our RAG application to LangSmith as a new dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZZ99RTzM_Nn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"   # ✅ Use Groq instead\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gwec7oiPM_No"
      },
      "outputs": [],
      "source": [
        "# from openai import OpenAI   ❌ Remove\n",
        "from groq import Groq   # ✅ Use Groq SDK\n",
        "\n",
        "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bWJp_xV5Oejt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "from langsmith import traceable\n",
        "\n",
        "# Initialize Groq client\n",
        "groq_client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# Traceable ensures runs are logged to LangSmith\n",
        "@traceable\n",
        "def langsmith_rag(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Simple RAG-style call using Groq as the LLM.\n",
        "    \"\"\"\n",
        "\n",
        "    # You can extend this by adding retrieved context before the question\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions clearly.\"},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-120b\",   # Groq model (fast + cost-efficient)\n",
        "        messages=messages,\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content   # ✅ correct\n",
        "\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE2gsWJgM_No",
        "outputId": "1798438d-6893-44e2-8829-d9739e477330"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'example_ids': ['3f5af433-5a4e-4e61-a355-f6ac78d98a7c',\n",
              "  '77eb4d8d-5621-46f6-be4e-3fa8c235d0eb',\n",
              "  '8ae3fc92-2b18-4ea9-ba01-ed1618cc5e3f',\n",
              "  '4b66882f-a29c-4d72-8419-120270bed93a',\n",
              "  '225bc0a7-9c8f-4e61-925a-7e974bd34780',\n",
              "  '2c53c635-bf6d-427b-b1a6-0812fd1875b3',\n",
              "  '62d9d7e2-2b18-4a24-a159-9f64bdbb3d20',\n",
              "  'c8ad1754-24d9-4da4-bbe4-5fe3460d0b46',\n",
              "  '3aeef2d3-aaaa-41ad-bf5f-bafdc8eff56a',\n",
              "  '88b214f4-9110-483f-ae41-aaf2db04ea04'],\n",
              " 'count': 10}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "example_inputs = [\n",
        "(\"How do I set up tracing to LangSmith if I'm using LangChain?\", \"To set up tracing to LangSmith while using LangChain, you need to set the environment variable `LANGSMITH_TRACING` to 'true'. Additionally, you must set the `LANGSMITH_API_KEY` environment variable to your API key. By default, traces will be logged to a project named \\\"default.\\\"\"),\n",
        "(\"How can I trace with the @traceable decorator?\", \"To trace with the @traceable decorator in Python, simply decorate any function you want to log traces for by adding `@traceable` above the function definition. Ensure that the LANGSMITH_TRACING environment variable is set to 'true' to enable tracing, and also set the LANGSMITH_API_KEY environment variable with your API key. By default, traces will be logged to a project named \\\"default,\\\" but you can configure it to log to a different project if needed.\"),\n",
        "(\"How do I pass metadata in with @traceable?\", \"You can pass metadata with the @traceable decorator by specifying arbitrary key-value pairs as arguments. This allows you to associate additional information, such as the execution environment or user details, with your traces. For more detailed instructions, refer to the LangSmith documentation on adding metadata and tags.\"),\n",
        "(\"What is LangSmith used for in three sentences?\", \"LangSmith is a platform designed for the development, monitoring, and testing of LLM applications. It enables users to collect and analyze unstructured data, debug issues, and create datasets for testing and evaluation. The tool supports various workflows throughout the application development lifecycle, enhancing the overall performance and reliability of LLM applications.\"),\n",
        "(\"What testing capabilities does LangSmith have?\", \"LangSmith offers capabilities for creating datasets of inputs and reference outputs to run tests on LLM applications, supporting a test-driven approach. It allows for bulk uploads of test cases, on-the-fly creation, and exporting from application traces. Additionally, LangSmith facilitates custom evaluations to score test results, enhancing the testing process.\"),\n",
        "(\"Does LangSmith support online evaluation?\", \"Yes, LangSmith supports online evaluation as a feature. It allows you to configure a sample of runs from production to be evaluated, providing feedback on those runs. You can use either custom code or an LLM as a judge for the evaluations.\"),\n",
        "(\"Does LangSmith support offline evaluation?\", \"Yes, LangSmith supports offline evaluation through its evaluation how-to guides and features for managing datasets. Users can manage datasets for offline evaluations and run various types of evaluations, including unit testing and auto-evaluation. This allows for comprehensive testing and improvement of LLM applications.\"),\n",
        "(\"Can LangSmith be used for finetuning and model training?\", \"Yes, LangSmith can be used for fine-tuning and model training. It allows you to capture run traces from your deployment, query and filter this data, and convert it into a format suitable for fine-tuning models. Additionally, you can create training datasets to keep track of the data used for model training.\"),\n",
        "(\"Can LangSmith be used to evaluate agents?\", \"Yes, LangSmith can be used to evaluate agents. It provides various evaluation strategies, including assessing the agent's final response, evaluating individual steps, and analyzing the trajectory of tool calls. These methods help ensure the effectiveness of LLM applications.\"),\n",
        "(\"How do I create user feedback with the LangSmith sdk?\", \"To create user feedback with the LangSmith SDK, you first need to run your application and obtain the `run_id`. Then, you can use the `create_feedback` method, providing the `run_id`, a feedback key, a score, and an optional comment. For example, in Python, it would look like this: `client.create_feedback(run_id, key=\\\"feedback-key\\\", score=1.0, comment=\\\"comment\\\")`.\"),\n",
        "]\n",
        "\n",
        "client = Client()\n",
        "# TODO: Fill in dataset id\n",
        "dataset_id = \"ecf95159-4a0b-4e91-9087-c4f048dec756\"\n",
        "\n",
        "# Prepare inputs and outputs for bulk creation\n",
        "inputs = [{\"question\": input_prompt} for input_prompt, _ in example_inputs]\n",
        "outputs = [{\"output\": output_answer} for _, output_answer in example_inputs]\n",
        "\n",
        "client.create_examples(\n",
        "  inputs=inputs,\n",
        "  outputs=outputs,\n",
        "  dataset_name=\"Golden Dataset\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-LO-BUXOSJi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cALyn3BvM_Nq"
      },
      "source": [
        "## Submitting another Trace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KgHYLy1M_Nq"
      },
      "source": [
        "I've moved our RAG application definition to `app.py` so we can quickly import it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7iVs497M_Nq",
        "outputId": "0e96cec2-d1e3-413a-bbf2-fa1e08f92dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Below is a **step‑by‑step guide** for wiring LangChain (Python) to **LangSmith** so that every LLM call, chain run, tool use, and agent step shows up in your LangSmith dashboard.\n",
            "\n",
            "---\n",
            "\n",
            "## 1️⃣ Install the required packages\n",
            "\n",
            "```bash\n",
            "# Core LangChain + LLM provider (e.g. OpenAI)\n",
            "pip install langchain openai\n",
            "\n",
            "# LangSmith SDK (includes the callback handler)\n",
            "pip install langsmith\n",
            "```\n",
            "\n",
            "> **Tip:** If you are already using `langchain[all]` you’ll get `langsmith` automatically, but it never hurts to run the explicit install.\n",
            "\n",
            "---\n",
            "\n",
            "## 2️⃣ Get your LangSmith credentials\n",
            "\n",
            "1. Log in to https://smith.langchain.com (or your self‑hosted instance).  \n",
            "2. Click **Settings → API Keys** → **Create new key**.  \n",
            "3. Copy the **API key** (e.g. `sk-xxxxxx`).  \n",
            "\n",
            "You’ll also need a **project name** (or you can let LangSmith create one on‑the‑fly).\n",
            "\n",
            "---\n",
            "\n",
            "## 3️⃣ Configure the environment (recommended)\n",
            "\n",
            "Set the variables **once** in your environment (Docker, `.env`, CI, etc.):\n",
            "\n",
            "```bash\n",
            "export LANGCHAIN_API_KEY=\"sk-xxxxxx\"          # <-- your LangSmith API key\n",
            "export LANGCHAIN_TRACING_V2=\"true\"            # turn on v2 tracing (default now)\n",
            "export LANGCHAIN_PROJECT=\"my-awesome-project\" # optional – name of the project\n",
            "export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"  # optional, only for self‑hosted\n",
            "```\n",
            "\n",
            "LangChain reads these variables automatically.  \n",
            "If you prefer to set them inside Python, you can use `os.getenv` or `langsmith.set_api_key()` (see below).\n",
            "\n",
            "---\n",
            "\n",
            "## 4️⃣ Attach the LangSmith callback handler to LangChain\n",
            "\n",
            "LangSmith provides a **single callback handler** that can be added to any LangChain component (LLM, chain, agent, tool, etc.).  \n",
            "\n",
            "### 4.1 The simplest way – global configuration\n",
            "\n",
            "```python\n",
            "from langchain.globals import set_debug\n",
            "from langsmith import traceable\n",
            "\n",
            "# Enable LangSmith tracing globally (reads env vars above)\n",
            "set_debug(True)          # optional: prints trace URLs while you run\n",
            "```\n",
            "\n",
            "When `LANGCHAIN_TRACING_V2` is `\"true\"` and a valid API key is present, **every** LangChain object you create after this line will automatically emit traces.\n",
            "\n",
            "### 4.2 Explicit handler (useful for fine‑grained control)\n",
            "\n",
            "```python\n",
            "from langchain.callbacks import CallbackManager\n",
            "from langsmith import LangChainTracer\n",
            "\n",
            "# Create a tracer instance (you can pass your own project name, tags, etc.)\n",
            "tracer = LangChainTracer(\n",
            "    project_name=\"my-awesome-project\",   # overrides env var if given\n",
            "    tags=[\"demo\", \"langchain\"],          # optional list of tags applied to every run\n",
            ")\n",
            "\n",
            "# Build a CallbackManager that only contains the LangSmith tracer\n",
            "callback_manager = CallbackManager([tracer])\n",
            "```\n",
            "\n",
            "Now you pass `callback_manager` to any component that accepts a `callback_manager` argument.\n",
            "\n",
            "---\n",
            "\n",
            "## 5️⃣ Example: OpenAI LLM + Simple Chain\n",
            "\n",
            "```python\n",
            "import os\n",
            "from langchain.llms import OpenAI\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.chains import LLMChain\n",
            "\n",
            "# --- 1️⃣ Set up the tracer (if you didn't rely on env vars) ---\n",
            "from langsmith import LangChainTracer\n",
            "tracer = LangChainTracer(project_name=\"tracing-demo\")\n",
            "\n",
            "# --- 2️⃣ Build the chain with the tracer ---\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"question\"],\n",
            "    template=\"You are a helpful assistant. Answer in one sentence: {question}\",\n",
            ")\n",
            "\n",
            "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0)   # <-- any LangChain LLM works\n",
            "chain = LLMChain(\n",
            "    llm=llm,\n",
            "    prompt=prompt,\n",
            "    # attach the callback manager (the tracer) to the chain\n",
            "    callback_manager=CallbackManager([tracer]),\n",
            ")\n",
            "\n",
            "# --- 3️⃣ Run it ---\n",
            "result = chain.invoke({\"question\": \"What is the capital of France?\"})\n",
            "print(result[\"text\"])\n",
            "```\n",
            "\n",
            "When you run this script, you’ll see a **URL printed** (if `set_debug(True)` is on) like:\n",
            "\n",
            "```\n",
            "LangSmith trace URL: https://smith.langchain.com/o/trace/abcd1234\n",
            "```\n",
            "\n",
            "Open that link – you’ll see:\n",
            "\n",
            "* LLM request & response (prompt, token usage, latency)\n",
            "* Prompt template rendered\n",
            "* Metadata (project, tags, run ID)\n",
            "* Any errors or retries\n",
            "\n",
            "---\n",
            "\n",
            "## 6️⃣ Using LangSmith with Agents, Tools, and Retrievers\n",
            "\n",
            "LangChain agents, retrievers, memory, etc. all accept a `callback_manager`.  \n",
            "Just reuse the same `CallbackManager([tracer])` you created above.\n",
            "\n",
            "### 6.1 Example – Conversational Retrieval QA\n",
            "\n",
            "```python\n",
            "from langchain.vectorstores import FAISS\n",
            "from langchain.embeddings import OpenAIEmbeddings\n",
            "from langchain.chains import ConversationalRetrievalChain\n",
            "from langchain.memory import ConversationBufferMemory\n",
            "\n",
            "# Vectorstore & retriever (any will do)\n",
            "embeddings = OpenAIEmbeddings()\n",
            "vectorstore = FAISS.from_texts(\n",
            "    [\"Paris is the capital of France.\", \"Berlin is the capital of Germany.\"],\n",
            "    embeddings,\n",
            ")\n",
            "\n",
            "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
            "\n",
            "# LLM + memory\n",
            "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
            "\n",
            "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
            "qa_chain = ConversationalRetrievalChain.from_llm(\n",
            "    llm,\n",
            "    retriever,\n",
            "    memory=memory,\n",
            "    # inject tracer for the whole agent run\n",
            "    callback_manager=CallbackManager([tracer]),\n",
            ")\n",
            "\n",
            "# Run a couple of turns\n",
            "print(qa_chain({\"question\": \"What is the capital of Germany?\"})[\"answer\"])\n",
            "print(qa_chain({\"question\": \"And of France?\"})[\"answer\"])\n",
            "```\n",
            "\n",
            "All **retriever calls, LLM calls, and memory updates** will be captured under a single top‑level trace with nested sub‑spans, making it easy to see the whole conversational flow.\n",
            "\n",
            "---\n",
            "\n",
            "## 7️⃣ Customizing Traces\n",
            "\n",
            "| Feature | How to set |\n",
            "|---------|------------|\n",
            "| **Run name** (appears in the UI) | `tracer = LangChainTracer(project_name=\"myproj\", run_name=\"my‑run\")` |\n",
            "| **Tags** (filtering & grouping) | `tracer = LangChainTracer(tags=[\"experiment‑1\", \"gpt‑4\"])` |\n",
            "| **Metadata** (key/value pairs) | `tracer = LangChainTracer(extra={\"dataset\": \"squad\", \"model_version\": \"v1\"})` |\n",
            "| **Disable tracing for a single call** | Pass `run_manager=None` or set `callbacks=[]` on that component. |\n",
            "| **Add a callback at runtime** | `chain.run(..., callbacks=[tracer])` |\n",
            "\n",
            "---\n",
            "\n",
            "## 8️⃣ Verifying that tracing works\n",
            "\n",
            "1. **Run a script** that contains at least one LLM call.  \n",
            "2. Look for a printed URL or check the **“Traces”** tab in the LangSmith UI.  \n",
            "3. If you see no traces:  \n",
            "\n",
            "   * Ensure `LANGCHAIN_API_KEY` (or `LangChainTracer(api_key=…)`) is correct.  \n",
            "   * Confirm `LANGCHAIN_TRACING_V2` is `\"true\"` (or `tracer = LangChainTracer(..., tracing_enabled=True)`).  \n",
            "   * Make sure your network can reach `api.smith.langchain.com` (or your self‑hosted endpoint).  \n",
            "   * Verify that the callback manager you passed actually contains the tracer (e.g., `print(callback_manager.handlers)`).\n",
            "\n",
            "---\n",
            "\n",
            "## 9️⃣ Quick‑fire reference cheat‑sheet\n",
            "\n",
            "```python\n",
            "# 1️⃣ Env vars (recommended)\n",
            "export LANGCHAIN_API_KEY=\"sk-…\"\n",
            "export LANGCHAIN_TRACING_V2=\"true\"\n",
            "export LANGCHAIN_PROJECT=\"my-project\"\n",
            "\n",
            "# 2️⃣ In Python (optional overrides)\n",
            "from langsmith import LangChainTracer, set_api_key\n",
            "\n",
            "set_api_key(\"sk-…\")                         # same as env var\n",
            "tracer = LangChainTracer(\n",
            "    project_name=\"my-project\",\n",
            "    tags=[\"demo\"],\n",
            "    extra={\"owner\": \"alice\"},\n",
            ")\n",
            "\n",
            "# 3️⃣ Attach to any component\n",
            "from langchain.callbacks import CallbackManager\n",
            "callback_manager = CallbackManager([tracer])\n",
            "\n",
            "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0, callback_manager=callback_manager)\n",
            "chain = LLMChain(llm=llm, prompt=prompt, callback_manager=callback_manager)\n",
            "\n",
            "# 4️⃣ Run\n",
            "result = chain.invoke({\"question\": \"…\"})\n",
            "print(result)\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "## 10️⃣ Using LangChain **v0.2+** (the newest API)\n",
            "\n",
            "If you are on the newest `langchain-core` (>=0.2.0), the pattern is the same but the import paths have changed:\n",
            "\n",
            "```python\n",
            "from langchain_core.prompts import PromptTemplate\n",
            "from langchain_openai import ChatOpenAI\n",
            "from langchain import LLMChain\n",
            "from langchain.callbacks import CallbackManager\n",
            "from langsmith import LangChainTracer\n",
            "\n",
            "tracer = LangChainTracer(project_name=\"new-api\")\n",
            "cb_manager = CallbackManager([tracer])\n",
            "\n",
            "prompt = PromptTemplate.from_template(\n",
            "    \"Answer the question in 1 sentence: {question}\"\n",
            ")\n",
            "\n",
            "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, callbacks=cb_manager)\n",
            "\n",
            "chain = LLMChain(prompt=prompt, llm=llm, callbacks=cb_manager)\n",
            "print(chain.invoke({\"question\": \"Who wrote Macbeth?\"}))\n",
            "```\n",
            "\n",
            "The **only difference** is the new module names (`langchain_openai`, `langchain_core`), but the tracing logic stays identical.\n",
            "\n",
            "---\n",
            "\n",
            "## 🎉 Summary\n",
            "\n",
            "1. **Install** `langsmith` (and LangChain).  \n",
            "2. **Set** `LANGCHAIN_API_KEY` and `LANGCHAIN_TRACING_V2=true` (or instantiate `LangChainTracer`).  \n",
            "3. **Create** a `LangChainTracer` (or rely on the global auto‑tracing).  \n",
            "4. **Pass** the tracer via a `CallbackManager` (or let LangChain pick it up automatically).  \n",
            "5. **Run** your LangChain app – every LLM call, chain, agent step, tool, and memory operation is now visible in LangSmith.\n",
            "\n",
            "That’s all you need to get full‑fidelity observability for your LangChain workloads. Happy tracing! 🚀\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Ask a question, this will:\n",
        "# 1. Retrieve context from vector DB (Groq embeddings)\n",
        "# 2. Call Groq LLM (model you set)\n",
        "# 3. Trace everything into LangSmith\n",
        "question = \"How do I set up tracing to LangSmith if I'm using LangChain?\"\n",
        "answer = langsmith_rag(question)\n",
        "\n",
        "print(\"Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZHv_O75iPpXr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "b-tpqL4Pxt6D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bbYiSO8OM_Nr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c79be197",
        "outputId": "1c8c0a9c-28c1-4439-faf5-3b79ca4a7850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.34)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.77 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.3.77)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.108.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.11.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.104.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain-openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.104.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.77->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=0.3.77->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U langchain-openai"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ls-academy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
